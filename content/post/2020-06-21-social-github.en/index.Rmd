---
title: Social activities on GitHub
author: Llu√≠s Revilla Sancho
date: '2020-06-21'
slug: social-github
categories:
  - r
tags:
  - GitHub
  - r
  - API
authors: []
description: ''
editor_options:
  chunk_output_type: console
featured: no
image:
  caption: ''
  focal_point: ''
subtitle: ''
summary: 'Presenting the socialGH package to retrieve information from GitHub.'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

On my [last post](http://llrs.dev/post/2020/06/bioconductor-submission.html) I explored the Bioconductor submissions using  [`{gh}`](https://cran.r-project.org/package=gh) to retrieve some data.
After some feedback from the Bioconductor community I realized I should download other kind of data to improve my analysis on the reviews. 

To make this I developed a new package to retrieve information from GitHub. 

## socialGH

[This package](https://github.com/llrs/socialGH) based on [`{gh}`](https://cran.r-project.org/package=gh), allows to retrieve, data from Github.

You can install it with 

```r
remotes::install_github("llrs/socialGH")
```

Basically pulls the data in list format and transforms it into a data.frame in order to be able to do analysis, filter it or analyze it.

```{r load, warning=FALSE}
library("socialGH")
library("tidyverse")
```

It allows to selective download comments, pull requests, issues, events, labels and the timeline of an issue. 

With the issues we can see the labels, how many coments and many information:

```{r issues, warning=FALSE}
issues_blog <- get_issues("llrs/blogR")
dim(issues_blog)
colnames(issues_blog)
# Labels used
issues_blog %>% 
  pull(label) %>% 
  unlist(FALSE, FALSE) %>% 
  table()
count(issues_blog, state)
count(issues_blog, n_comments)
```

However, it doesn't retrieve each comment of an issue. 

```{r comments}
# Issues with comments
issues_blog %>% 
  filter(n_comments > 0) %>% 
  pull(id)

comments <- get_comments("llrs/blogR")
dim(comments)
colnames(comments)
count(comments, association)
```

We can see that I was the only one writing on the issues and we already retrieved the text of the comments.

We can also look for events on issues:
```{r events}
events <- get_events("llrs/blogR", 23)
count(events, event)
```

On all the functions you can provide a number of the issue and you'll retrieve the information just for that issue. If you don't provide an issue it will search the whole repository:

```{r events2}
events <- get_events("llrs/blogR")
count(events, event)
```

However it is better if we look to the timeline of an issue:, which downloads each comment of the issues.

```{r timeline}
gt <- get_timelines("llrs/blogR", 23)
gt[, c("label", "event", "created", "association", "actor")]
```

With timeline we don't get the initial information of when the issue was created and we'll need to call `get_issue("llrs/blogR", 23)` to know that. 
Here I did omit the text of the comment to make it readable, but we can see what has been happening and by who or who is affecting. 

## Learning

Developing this package I learned more about the `{gh}` package (In the previous blog I wrote manually the calls to different pages, which later on I discovered it is automatically handled by `{gh}`). 
And learned that the different accept headers have influenced on the total information returned (and that you cannot pass several accept headers at the same time).  
Hope to learn more about the R community that is using Github as a way to help each other, improve packages and process. 

### Reproducibility

<details>
```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
sessioninfo::session_info()
```
</details>
