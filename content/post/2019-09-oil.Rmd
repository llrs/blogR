---
title: Consumption of fuel by our car
author: Llu√≠s Revilla Sancho
date: '2019-09-15'
slug: oil
categories:
  - r
tags:
  - r
  - oil
authors: []
description: ''
featured: no
image:
  caption: ''
  focal_point: ''
subtitle: ''
summary: ''
editor_options: 
  chunk_output_type: console
---

## Introduction

For years my dad has been taking the notes of the gas stations and annotating 
the km done by the car until that moment and the km since last stop to refill.

Now, he suspects that the car is not as efficient as it was so he want to check the oil consumption.


## The data


I manually had to annotate the data, but I uploaded here:

```{r load}
library("here")
g <- read.csv(here("content", "gasolina.csv"))
```

It is tidy but real data; some errors, missing values are present.
I start with a visualization:

```{r}
library("ggplot2")
library("dplyr")
library("lubridate")
g_clean <- g %>% 
  mutate(Date = dmy(Date)) %>% 
  arrange(Date)

ggplot(g_clean) +
  geom_point(aes(Date, total_km, size = L)) +
  theme_bw()
```

There are some values that are plain wrong, all of a sudden the car had less kilometers!
Also we can see that there are some unit with more than 15000 Liters on the tank!
We can check for other inconsistences.

```{r summary}
summary(g_clean)
```

We can see on the summary that there is a data point at the the 2020. 
In reality is a bill that had no year, so it is corrected by lubridate to 2020.

On the positive side it seems like the tank is bigger than expected and it can have more than 40 liters.
We need to check the manual, but this came as a suprise to my father.

## Data cleaning

Checking the originally data (make bakups or store the original data), 
I found that instead of 19349 Liters it has been 19.49 L and the other outliers are also my mistake typing.

```{r corrections}
g_clean$L[g_clean$L > 100] <- 19.49
g_clean$Date[g_clean$Date > "2020/01/01"] <- "2018/01/21"
g_clean$Last_km[g_clean$Last_km > 1000] <- 597.6
g_clean$total_km[g_clean$total_km < 20000 & 
                   year(g_clean$Date) < 2018 & 
                   year(g_clean$Date) > 2016 &
                   !is.na(g_clean$total_km)] <- 39065
g_clean$total_km[g_clean$total_km < 7000 & 
                   year(g_clean$Date) >= 2016 &
                   !is.na(g_clean$total_km)] <- 16977
# A duplicate with a wrong date
g_clean <- g_clean[g_clean$Date != "2019/03/26", ]
g_clean$Date[g_clean$Date == "2019/06/26" & 
                   g_clean$total_km < 60000] <- "2019/03/26"
g_clean$total_km[g_clean$Date == "2016/06/26"] <- 18450
g_clean$total_km[g_clean$Date == "2016/06/26"] <- 18450
g_clean <- arrange(g_clean, Date)
```

After this manual curration we can check the plot again

```{r replot}
ggplot(g_clean) +
  geom_point(aes(Date, total_km, size = L)) +
  theme_bw()
```

Now the plot is much better.

## Insihgts


### Efficiency
Now that the data has been corrected to the best of my hability we can start comparing liters and km:

```{r calc1}
g_clean2 <- g_clean %>% 
  mutate(
    Last_km = round(Last_km),
    total_l = cumsum(coalesce(L, 0)),
    ratio = total_km/total_l
  )
```

In the last block we set up the data to make the comparison. 
Now we want to model the data, we will use [broom]():

```{r model}
library("broom")
model <- lm(total_km ~ total_l, data = filter(g_clean2, !is.na(total_km)))

tidy(model)
glance(model)
```

Note that with 0 L we cannot have traveled. 
As in this case the first date has already some km done, we allow the intercept.
Otherwise, we could set up a linear model without interception. 

This means that every `r tidy(model)[2, 2, drop = TRUE]` km the car spends 1L, 
and it seems quite consistent over the years, but we missed around `r round(tidy(model)[1, 2, drop = TRUE])` L before we started collecting data:

```{r plot2}
g_clean2 %>% 
  filter(!is.na(total_km)) %>% 
  ggplot(aes(total_l, total_km)) +
  geom_smooth(method = lm, col = "red") +
  geom_point() +
  theme_bw()
```

Now we can look if there are some years that it was worse.
To do so we look at the residuals and the distribution of them:

```{r residuals}
augment(model) %>% 
  cbind(filter(g_clean2, !is.na(total_km))[ ,-c(3, 6)]) %>% 
  ggplot() +
  geom_point(aes(Date, .resid)) +
  geom_smooth(aes(Date, .resid), method = "glm") +
  theme_minimal()
```

As we can see there is not a pattern here. 
If the car were more efficient at the beginning we would see a trend that the residuals would increase.  
But let's check it :

```{r}
augment(model) %>% 
  cbind(filter(g_clean2, !is.na(total_km))[ ,-c(3, 6)]) %>% 
  lm(.resid ~ Date, data = .) %>% 
  glance()
```

As we can see above we can't say there is a linear tread to increased consumption with time.

### Refills

Another interesting question is how many refills did we miss. 

```{r refills1}
library("ggbeeswarm")
g_clean2 %>% 
  ggplot() +
  geom_beeswarm(aes("Refills", Last_km)) +
  labs(x = element_blank()) +
  theme_bw()

g_clean2 %>% 
  summarise(median = median(Last_km, na.rm = TRUE),
            mean = round(mean(Last_km, na.rm = TRUE)))
max(g_clean2$total_km, na.rm = TRUE)/c(541, 534)
```

We can round this to 123 refills. 
So apparently  we missed `r 123-106` refills. 
Or did I miss something here?

Let's check how many refills are already missing from the data we have:


```{r refills}
g_clean3 <- g_clean2 %>% 
  mutate(
    cum_km = cumsum(coalesce(Last_km, 0)),
    km_at_last_refill = total_km - Last_km,
    diff = total_km - km_at_last_refill[-1],
    diff = if_else(is.na(diff), 0, diff),
    refills_missed = round(abs(diff)/median(Last_km, na.rm = TRUE))
)
sum(g_clean3$refills_missed)
```

To that amount we need to add the first 8171 km that we don't have information of the refills.
But those are around `r round(8171/531)-1`, plus the one we already have come close to those 17 we estimated.
The difference is because there have been some smaller refills. 

### Long and short distances

If we look at it as a time series, we might notice some jumps. 
This might be due to a long trip. 
We can see if we can extract if there are some anomalies on the serie:

```{r anomalize}
library("anomalize")
g_clean2 %>% 
  filter(!is.na(total_km)) %>% 
  as_tibble() %>% 
  time_decompose(total_km, method = "stl") %>% 
  anomalize(remainder, method = "iqr") %>% 
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.25) +
  labs(title = "Refill Anomalies", subtitle = "STL + IQR Methods") 
```


```{r}
g_clean2 %>% 
  filter(!is.na(total_km)) %>% 
  time_frequency(period = "auto")
g_clean2 %>% 
  filter(!is.na(total_km)) %>% 
  time_trend(period = "auto")

al <- g_clean2 %>% 
  filter(!is.na(total_l)) %>% 
  as_tibble() %>% 
  time_decompose(total_l, method = "stl", trend = "1 year", frequency = "1 months") %>% 
  anomalize(remainder, method = "iqr") %>% 
  plot_anomaly_decomposition() +
  labs(title = "total l")
ak <- g_clean2 %>% 
  filter(!is.na(total_km)) %>% 
  as_tibble() %>% 
  time_decompose(total_km, method = "stl", trend = "1 year", frequency = "1 months") %>% 
  anomalize(remainder, method = "iqr") %>% 
  plot_anomaly_decomposition() +
  labs(title = "total km")

ar <- g_clean2 %>%
  filter(!is.na(ratio)) %>% 
  as_tibble() %>% 
  time_decompose(ratio, method = "stl") %>% 
  anomalize(remainder, method = "iqr") %>% 
  plot_anomaly_decomposition() +
  labs(title = "total km/l")
library("patchwork")
al + ak + plot_annotation(title = "Anomalies")
```


```{r bibsetup, echo=FALSE, message=FALSE, warning=FALSE}
## Load knitcitations with a clean bibliography
library('knitcitations')
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
pi <- sessioninfo::package_info()
packages <- c(pi$package[pi$attached], 'knitcitations')
l <- lapply(packages, function(x){citation(x)[1]})
bib <- c(l, 'blogdown' = citation('blogdown')[2], 
         'sessioninfo' = citation('sessioninfo'))
```

### References

```{r results = 'asis', echo = FALSE, cache = FALSE}
bibliography(bib, style = 'html')
```

### Reproducibility

<details>
```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
sessioninfo::session_info()
```
<details>
