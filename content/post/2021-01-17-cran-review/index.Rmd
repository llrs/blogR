---
title: CRAN review
author: Llu√≠s Revilla Sancho
date: '2021-01-17'
categories:
  - r
  - CRAN
tags:
  - CRAN
  - R
  - reviews
slug: CRAN-review
editor_options:
  chunk_output_type: console
featured: no
draft: false
image:
  caption: ''
  focal_point: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

I've been doing some [analysis on the review submissions](https://llrs.dev/tags/reviews/) of several projects of R.
However, till recently I couldn't analyze the CRAN submission.
There was [cransays](https://github.com/lockedata/cransays)' package to check package submissions which on the online documentation provided a [dashboard](https://lockedata.github.io/cransays/articles/dashboard.html) which updated each hour.
Since 2020/09/12 the status of the queues and folders of submissions are saved on a branch.
Using this information and basing in [script provided by Tim Taylor](https://github.com/tjtnew/newbies) I'll check how are the submissions on CRAN handled.

I'll look at the [CRAN queue](#cran-load), I'll explore some [time patterns](#time-patterns) and also check the meaning of those [subfolder](#subfolder).
Later I'll go to a more [practical information](#information-for-submitters) for people submitting a package.
Lastly, we'll see how hard is the job of the CRAN team by looking at the reliability of the [Github action used](#GHAR).

Before all this we need preliminary work to download the data:

```{r prepare, eval = FALSE}
# Downloading the cransays repository branch history
download.file("https://github.com/lockedata/cransays/archive/history.zip", 
              destfile = "static/cransays-history.zip")
path_zip <- here::here("static", "cransays-history.zip") 
# We unzip the files to read them
dat <- unzip(path_zip, exdir = "static")
csv <- dat[grepl("*.csv$", x = dat)]
f <- lapply(csv, read.csv)
m <- function(x, y) {
  merge(x, y, sort = FALSE, all = TRUE)
}
updates <- Reduce(m, f) # Merge all files (Because the file format changed)
write.csv(updates, file = "static/cran_till_now.csv",  row.names = FALSE)
# Clean up
unlink("static/cransays-history/", recursive = TRUE)
unlink("static/cransays-history.zip", recursive = TRUE)
```

Once we have the data we can load it:

```{r load}
library("tidyverse")
library("lubridate")
path_file <- here::here("static", "cran_till_now.csv")
cran_submissions <- read.csv(path_file)
theme_set(theme_minimal()) # For plotting
col_names <- c("package", "version", "snapshot_time", "folder", "subfolder")
cran_submissions <- cran_submissions[, col_names]
```


The period we are going to analyze is from the beginning of the records till `r as.Date(max(cran_submissions$snapshot_time))`.
It includes some well earned holiday time for the CRAN team, during which submissions were not possible.

I've read some comments on the inconsistencies of where the holidays of the CRAN teams are reported and I couldn't find it for previous years.

For the 4 months we are analyzing which only has one holiday period I used a [screenshot](https://twitter.com/krlmlr/status/1346005787668336640) found on twitter.

```{r cran-holidays}
holidays <- data.frame(
  start = as.POSIXct("18/12/2020", format = "%d/%m/%Y", tz = "UTC"), 
  end = as.POSIXct("04/01/2021", format = "%d/%m/%Y", tz = "UTC")
)
```


## Cleaning the data

After preparing the files in one big file we can load and work with it.
First steps, check the data and prepare it for what we want:

```{r prepare}
# Use appropiate class
cran_submissions$snapshot_time <- as.POSIXct(cran_submissions$snapshot_time,
                                             tz = "UTC")
# Fix subfolders structure
cran_submissions$subfolder[cran_submissions$subfolder %in% c("", "/")] <- NA
# Remove files or submissions without version number
cran_submissions <- cran_submissions[!is.na(cran_submissions$version), ]
cran_submissions <- distinct(cran_submissions, 
                             snapshot_time, folder, package, version, subfolder,
                             .keep_all = TRUE)
```

After loading and a preliminary cleanup, set date format, homogenize folder format, remove submissions that are not packages (yes there are pdf and other files on the queue), and remove duplicates we can start.

As always start with some checks of the data. 
Note: I should follow more often this advice myself, as this is the last section I'm writing on the post. 

```{r duplicated-packages}
packges_multiple_versions <- cran_submissions %>% 
  group_by(package, snapshot_time) %>% 
  summarize(n = n_distinct(version)) %>% 
  filter(n != 1) %>% 
  distinct(package) %>% 
  pull(package)
```

There are `r length(packges_multiple_versions)` packages with multiple versions at the same time on CRAN queue.

Perhaps because package are left in different folders (2 or even 3) at the same time:

```{r package-multiple-folders}
package_multiple <- cran_submissions %>% 
  group_by(snapshot_time, package) %>% 
  count() %>% 
  group_by(snapshot_time) %>% 
  count(n) %>% 
  filter(n != 1) %>% 
  summarise(n = sum(nn)) %>% 
  ungroup()
ggplot(package_multiple) +
  geom_point(aes(snapshot_time, n), size = 1) +
  geom_rect(data = holidays, aes(xmin = start, xmax = end, ymin = 0, ymax = 6),
            alpha = 0.25, fill = "red") +
  annotate("text", x = holidays$start + (holidays$end - holidays$start)/2, 
           y = 3.5, label = "CRAN holidays") +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
                   expand = expansion()) +
  scale_y_continuous(expand = expansion()) +
  labs(title = "Packages in multiple folders and subfolders", 
       x = element_blank(), y = element_blank())
```

This happens in `r nrow(package_multiple)` snapshots of `r n_distinct(cran_submissions$snapshot_time)`, probably due to the manual labor of the CRAN reviews.
I don't really know the cause of this, it could be an error on the script recording the data, copying the data around the server.
But perhaps this indicates further improvements and automatization of the process can be done.

```{r remove-duplicated-packges-version}
cran_submissions <- cran_submissions %>% 
  arrange(package, snapshot_time, version, folder) %>% 
  group_by(package, snapshot_time) %>% 
  mutate(n = 1:n()) %>% 
  filter(n == n()) %>% 
  ungroup() %>% 
  select(-n)
```

Now we removed ~3500 records of packages with two versions remaining on the queue.
Now we are ready to analyze more.

```{r remove-duplicated-packges-folder}
cran_submissions <- cran_submissions %>% 
  arrange(package, snapshot_time, folder) %>% 
  group_by(package, snapshot_time) %>% 
  mutate(n = 1:n()) %>% 
  filter(n == n()) %>% 
  ungroup() %>% 
  select(-n)
```

Last we want to know how many submissions (in this period did a package undergone:)

```{r submissions_cleanup}
diff0 <- structure(0, class = "difftime", units = "hours")
cran_submissions <- cran_submissions %>% 
  arrange(package, version, snapshot_time) %>% 
  group_by(package) %>% 
  # Packages last seen in queue less than 24 ago are considered same submission
  mutate(diff_time = difftime(snapshot_time,  lag(snapshot_time), units = "hour"),
         diff_time = if_else(is.na(diff_time), diff0, diff_time), # Fill NAs
         diff_v = version != lag(version),
         diff_v = ifelse(is.na(diff_v), TRUE, diff_v), # Fill NAs
         new_version = !near(diff_time, 1, tol = 24) & diff_v, 
         new_version = if_else(new_version == FALSE & diff_time == 0, 
                               TRUE, new_version),
         submission_n = cumsum(as.numeric(new_version))) %>%
  ungroup() %>% 
  select(-diff_time, -diff_v, -new_version)
```


## CRAN load {#cran-load}

We all know that CRAN is busy with updates to fix bugs, improve package, or with petitions to have new packages included on the repository.

A first plot we can make is showing the number of distinct packages on each moment:

```{r cran-queues}
cran_queue <- cran_submissions %>% 
  group_by(snapshot_time) %>% 
  summarize(n = n_distinct(package))
ggplot(cran_queue) +
  geom_rect(aes(xmin = start, xmax = end, ymin = 0, ymax = 230),
            alpha = 0.5, fill = "red", data = holidays) +
  annotate("text", x = holidays$start + (holidays$end - holidays$start)/2, 
           y = 150, label = "CRAN holidays") +
  geom_path(aes(snapshot_time, n)) +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
                   expand = expansion()) +
  scale_y_continuous(expand = expansion()) +
  labs(x = element_blank(), y = element_blank(), 
       title = "Packages on CRAN review process")

# Several version of the same packages in the same time.
# This makes this completley wrong in those cases. 
```

We can see that there are some ups and downs, and ranges between 200 to 50.

There are some instance were the number of package on the queue have a sudden drop and then a recovery to previous levels.
This might be an error on the CRAN system, or the cransays' script.
We can also see that people do not tend to rush and push the package before the holidays.
But clearly there is some build up of submissions after holidays, as the the highest packages on the queue is reached after holidays.

On the CRAN review process classifying package in folders seems to be part of the process:

```{r cran-submissions}
man_colors <- RColorBrewer::brewer.pal(8, "Dark2")
names(man_colors) <- unique(cran_submissions$folder)
cran_submissions %>% 
  group_by(folder, snapshot_time) %>% 
  summarize(packages = n_distinct(package)) %>% 
  ggplot() +
  geom_rect(data = holidays, aes(xmin = start, xmax = end, ymin = 0, ymax = 200),
            alpha = 0.25, fill = "red") +
  annotate("text", x = holidays$start + (holidays$end - holidays$start)/2, 
           y = 105, label = "CRAN holidays") +
  geom_path(aes(snapshot_time, packages, col = folder)) +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
                   expand = expansion()) +
  scale_y_continuous(expand = expansion()) +
  scale_color_manual(values = man_colors) +
  labs(x = element_blank(), y = element_blank(),
       title = "Packages by folder", col = "Folder") +
  theme(legend.position = c(0.6, 0.7))
```

The queue trend is mostly driven by newbies folder (which ranges between 25 and 150) and after holidays by the pretest folder.

Surprisingly when the queue is split by folder we don't see those sudden drops.
This might indicate that there is a clean up on some of the folders[^1].
What we clearly see is a clean up on holidays for all the folders, when almost all was cleared up.

[^1]: Or a problem with ggplot2 representing a sudden value that is much different from those around them.

Also the pretest folder might indicate that the package there are just updating.
As there isn't any later increase of other folders as we can see if we <details><summary>focus on the holidays and after them.</summary>

```{r cran-holidays-zoom}
cran_submissions %>% 
  group_by(folder, snapshot_time) %>% 
  summarize(packages = n_distinct(package)) %>% 
  filter(snapshot_time >= holidays$start) %>% 
  ggplot() +
  geom_path(aes(snapshot_time, packages, col = folder)) +
  geom_rect(data = holidays, aes(xmin = start, xmax = end, ymin = 0, ymax = 200),
            alpha = 0.25, fill = "red") +
  annotate("text", x = holidays$start + (holidays$end - holidays$start)/2, 
           y = 105, label = "CRAN holidays") +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "1 day", 
                   expand = expansion()) +
  scale_y_continuous(expand = expansion(), limits = c(0, NA)) +
  scale_color_manual(values = man_colors) +
  labs(x = element_blank(), y = element_blank(),
       title = "Holidays", col = "Folder") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = c(0.8, 0.7))
```

It seems like that on the 31st there was a clean up of some packages on the waiting list.
And we can see the increase of submissions on the first week of January.

</details>

## Time patterns {#time-patterns}

Some people has expressed they view to submit to CRAN when there are few packages on the queue.
Thus, looking when does this low moments happens could be relevant.
We can look for patterns on the queue:

-   [Day of the month](#day-month)
-   [Day of the week](#day-week)

Note: I have few to none experience with time series, so the following plots are just using the defaults of \`geom\_smooth\`, just omitting the holidays.

### By day of the month {#day-month}

Looking at folder:

```{r cran-monthly}
cran_times <- cran_submissions %>% 
  mutate(seconds = seconds(snapshot_time),
         month = month(snapshot_time),
         mday = mday(snapshot_time),
         wday = wday(snapshot_time, locale = "en_GB.UTF-8"),
         week = week(snapshot_time),
         date = as_date(snapshot_time))
cran_times %>% 
  arrange(folder, date, mday) %>% 
  filter(snapshot_time < holidays$start | snapshot_time  > holidays$end) %>% 
  group_by(folder, date, mday) %>% 
  summarize(packages = n_distinct(package),
            week = unique(week)) %>% 
  group_by(folder, mday) %>% 
  ggplot() +
  # geom_point(aes(mday, packages, col = fct_reorder2(folder, mday, packages, sum))) +
  geom_smooth(aes(mday, packages, col = folder)) +
  labs(x = "Day of the month", y = element_blank(), col = "Folder",
       title = "Evolution by month day") +
  scale_color_manual(values = man_colors) +
  coord_cartesian(ylim = c(0, NA), xlim = c(1, NA)) +
  scale_x_continuous(expand = expansion()) +
  scale_y_continuous(expand = expansion()) 
```

At the beginning and end of the month there is more variation on several folders (This could also be that there isn't information of the end of December and beginning of January).
There seems to be an increase of new packages towards the end of the month and later and increase on the human folder by the beginning of the month.

### By day of the week {#day-week}

I first thought about this, because I was curious if there is more submission on weekends (when aficionados and open source developers might have more time) or the rest of the week.

```{r cran-wday}
cran_times %>% 
  filter(snapshot_time < holidays$start | snapshot_time  > holidays$end) %>% 
  group_by(folder, date, wday) %>% 
  summarize(packages = n_distinct(package),
            week = unique(week)) %>% 
  group_by(folder, wday) %>% 
  ggplot() +
  # geom_point(aes(wday, packages, col = fct_reorder2(folder, wday, packages, sum))) +
  geom_smooth(aes(wday, packages, col = folder)) +
  labs(x = "Day of the week", y = "Packages", col = "Folder",
       title = "Evolution by week day") +
  scale_color_manual(values = man_colors) +
  scale_x_continuous(breaks = 1:7, expand = expansion()) +
  scale_y_continuous(expand = expansion(), limits = c(0, NA))
```

It doesn't seem to be a special pattern only on the human folder there is an increase on weekdays and a decrease on the weekend.
What we see is a rise towards the middle of the week of the packages on the pretest folder, indicating new packages submissions. 
However, note that reviewers might be on multiple time zones and this doesn't allow us to make hard conclusions on this data[^2].

[^2]: I don't know the list of the reviewers, just some folders with the initials of some R core members, and I haven't check their time zones. 

### Other folders

There are some folders that seem to be from [R Contributors](https://www.r-project.org/contributors.html).
We see that some packages are on these folders:

```{r subfolder-pattern}
cran_members <- c("LH", "GS", "JH")
cran_times %>% 
  filter(subfolder %in% cran_members) %>% 
  group_by(subfolder, snapshot_time) %>% 
  summarize(packages = n_distinct(package)) %>% 
  ggplot() +
  geom_smooth(aes(snapshot_time, packages, col = subfolder)) +
    labs(x = element_blank(), y = element_blank(), col = "Folder",
       title = "Packages on folders") +
  scale_y_continuous(expand = expansion(), breaks = 0:10) +
  coord_cartesian(y = c(0, NA))  +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
               expand = expansion(add = 2))
```

There doesn't seem to be any rule about using those folders or, the work was quick enough to not be recorded.

<details><summary>Looking for any temporal pattern doesn't seem to be worth it.</summary>

```{r subfolder-mday}
cran_times %>% 
  filter(subfolder %in% cran_members) %>% 
  group_by(subfolder, mday) %>% 
  summarize(packages = n_distinct(package)) %>% 
  ungroup() %>% 
  ggplot() +
  geom_smooth(aes(mday, packages, col = subfolder)) +
  labs(x = "Day of the month", y = element_blank(), col = "Subfolder",
       title = "Packages on subfolers by day of the month") +
  scale_y_continuous(expand = expansion()) +
  scale_x_continuous(expand = expansion(), breaks = c(1,7,14,21,29)) +
  coord_cartesian(ylim = c(0, NA))
```

Low number of packages and great variability (except on those that just have 1 package on the folder) on day of month.

```{r subfolder-wday, warning=FALSE}
cran_times %>% 
  filter(subfolder %in% cran_members) %>% 
  group_by(subfolder, wday) %>% 
  summarize(packages = n_distinct(package)) %>% 
  ungroup() %>% 
  ggplot() +
  geom_smooth(aes(wday, packages, col = subfolder)) +
  labs(x = "Day of the week", y = element_blank(), col = "Subfolder",
       title = "Evolution by week day") +
  scale_y_continuous(expand = expansion()) +
  scale_x_continuous(breaks = 1:7, expand = expansion()) +
  coord_cartesian(ylim =  c(0, NA))
```

There seem to be only 2 people usually working with their folders.
Suppose that there aren't a common set of rules the reviewers follow.

</details>

## Information for submitters {#information-for-submitters}

I've read lots of comments recently around CRAN submission.
However, with the few data available compared to open reviews on [Bioconductor](https://llrs.dev/2020/07/bioconductor-submissions-reviews/ "Analysis of Bioconductor reviews") and [rOpenSci](https://llrs.dev/2020/09/ropensci-submissions/ "Analysis of Bioconductor reviews") it is hard to answer them (See those related posts).
On Bioconductor and rOpenSci it is possible to see people involved, message from the reviewers and other interested parties, steps done to be accepted...

One of the big question we can provide information about with the data available is how long it will be a package on the queue:

```{r time-submission}
subm <- cran_times %>%
  group_by(package, submission_n) %>% 
  arrange(snapshot_time) %>% 
  select(package, version, submission_n, snapshot_time) %>% 
  filter(row_number() %in% c(1, last(row_number()))) %>% 
  arrange(package, submission_n)
```

There are `r sum(table(subm$package) == 1)` packages that are only seen once.
It might mean that it is an abandoned, delayed or rejected submissions like [knitcitations](https://github.com/cboettig/knitcitations/issues/107#issuecomment-736069214), other might be acceptances in less than an hour[^3].

[^3]: Which now I cannot find the evidence to link to.
    If anyone finds the tweet I would appreciate it.

```{r resubm}
rsubm <- subm %>% 
  filter(n_distinct(snapshot_time) %% 2 == 0) %>%
  select(-version) %>% 
  mutate(time = c("start", "end")) %>% 
  pivot_wider(values_from = snapshot_time, names_from = time) %>% 
  ungroup() %>% 
  mutate(r = row_number(), 
         time  =  round(difftime(end, start, units = "hour"), 0)) %>% 
  group_by(package) %>% 
  mutate(n_submission = 1:n()) %>% 
  ungroup()
lv <- levels(fct_reorder(rsubm$package, rsubm$start, .fun = min, .desc = FALSE))
ggplot(rsubm) +
  geom_rect(data = holidays, aes(xmin = start, xmax = end), 
            ymin = first(lv), ymax = last(lv), alpha = 0.5, fill = "red") +
  geom_linerange(aes(y = fct_reorder(package, start, .fun = min, .desc = FALSE),
                      x = start, xmin = start, xmax = end, 
                     col = as.factor(n_submission))) + 
  labs(x = element_blank(), y = element_blank(), title = 
         "Packages on the queue", col = "Submissions") +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
                   expand = expansion(add = 2)) +
  scale_colour_viridis_d() +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank(),
        legend.position = c(0.15, 0.7))
```

We can see the rapid growth of packages and that on this period some people submitted more than 5 times their packages in this period.

Also note that the definition used for submission is a package with different version number.
Some authors do change the version number when CRAN reviewers require changes before accepting the package on CRAN while others do not and only change the version number according to their release cycle.

Perhaps we could differentiate this by looking at the period between submissions but I think it won't be interesting.
As sometimes after a release there is soon a fast update to fix some bugs raised on the new features introduced.

Despite this differences we can see that most submissions are on the queue fairly few time:

```{r package-time-queue}
library("patchwork")
p1 <- rsubm %>% 
  group_by(package) %>% 
  summarize(time = sum(time)) %>% 
  ggplot() +
  geom_histogram(aes(time), bins = 100) +
  labs(title = "Packages total time on queue", x = "Hours", 
       y = element_blank()) +
  scale_x_continuous(expand = expansion()) +
  scale_y_continuous(expand = expansion())
p2 <- rsubm %>% 
  group_by(package) %>% 
  summarize(time = sum(time)) %>% 
  ggplot() +
  geom_histogram(aes(time), binwidth = 24) +
  coord_cartesian(xlim = c(0, 24*7)) +
  labs(subtitle = "Zoom", x = "Hours", y = element_blank()) +
  scale_x_continuous(expand = expansion(), breaks = seq(0, 24*7, by = 24)) +
  scale_y_continuous(expand = expansion()) +
  theme(panel.background = element_rect(colour = "white"))
p1 + inset_element(p2, 0.2, 0.2, 1, 1)
```

Also note that I found some packages that remained on the submission queue, and thus picked up by cransays, even after acceptance.

```{r submission-queue}
p1 <- rsubm %>% 
  group_by(package, submission_n) %>% 
  summarize(time = sum(time)) %>% 
  ggplot() +
  geom_histogram(aes(time), bins = 100) +
  labs(title = "Submission time on queue", x = "Hours", 
       y = element_blank()) +
  scale_x_continuous(expand = expansion()) +
  scale_y_continuous(expand = expansion())
p2 <- rsubm %>% 
  group_by(package, submission_n) %>% 
  summarize(time = sum(time)) %>%  
  ggplot() +
  geom_histogram(aes(time), binwidth = 24) +
  coord_cartesian(xlim = c(0, 24*7)) +
  labs(subtitle = "Zoom", x = "Hours", y = element_blank()) +
  scale_x_continuous(expand = expansion(), breaks = seq(0, 24*7, by = 24)) +
  scale_y_continuous(expand = expansion()) +
  theme(panel.background = element_rect(colour = "white"))
p1 + inset_element(p2, 0.2, 0.2, 1, 1)
```

By submission many more are shortly spanned.
Perhaps hinting that more testing should be done before or what to expect on the review should be more clear to the authors, or that they are approved very fast, or...

There are `r sum(table(subm$package) == 1)` packages that are only seen once.
It might mean that it is an abandoned/rejected submissions other might be acceptances in less than an hour.

```{r resubm2}
subm2 <- cran_times %>%
  group_by(package, submission_n, folder) %>% 
  arrange(snapshot_time) %>% 
  select(package, version, submission_n, snapshot_time, folder) %>% 
  filter(row_number() %in% c(1, last(row_number()))) %>% 
  arrange(submission_n)
rsubm2 <- subm2 %>% 
  filter(n_distinct(snapshot_time) %% 2 == 0) %>%
  mutate(time = c("start", "end")) %>% 
  pivot_wider(values_from = snapshot_time, names_from = time) %>% 
  ungroup() %>% 
  mutate(r = row_number(), 
         time  =  round(difftime(end, start, units = "hour"), 0)) %>% 
  ungroup() %>% 
  filter(!is.na(start), !is.na(end))
lv <- levels(fct_reorder(rsubm2$package, rsubm2$start, .fun = min, .desc = FALSE))
ggplot(rsubm2) +
  geom_rect(data = holidays, aes(xmin = start, xmax = end), 
            ymin = first(lv), ymax = last(lv), alpha = 0.5, fill = "red") +
  geom_linerange(aes(y = fct_reorder(package, start, .fun = min, .desc = FALSE),
                      x = start, xmin = start, xmax = end, col = folder)) + 
  labs(x = element_blank(), y = element_blank(), title = 
         "Packages on the queue") +
  scale_color_manual(values = man_colors) +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
               expand = expansion(add = 2)) +
  labs(col = "Folder") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        axis.text.y = element_blank(),
        legend.position = c(0.2, 0.7))
```

Looking at the submissions by the folder they are, looks like some packages take a long time to change folder. 
Some packages are recorded to be in just 1 folder and some other go through multiple folders:

```{r submissions-n-folders}
rsubm2 %>% 
  group_by(package, submission_n) %>% 
  summarize(n_folder = n_distinct(folder)) %>% 
  ggplot() + 
  geom_histogram(aes(n_folder)) +
  labs(title = "Folders by submission", x = element_blank(), 
       y = element_blank())
```

Most submissions end up in one folder, but some go up to 5 folders.

```{r submissions-folders}
subm_folder <- cran_times %>% 
  group_by(package, submission_n) %>% 
  distinct(folder) %>% 
  summarise(folder = list(folder)) %>% 
  ungroup() %>% 
  count(folder, sort = TRUE)
```

The most 5 common folders process of submissions are `r paste(unlist(lapply(subm_folder$folder[1:5], paste, collapse = ", ")), collapse = "; ")`.

Another way of seeing whether it is a right moment to submit your package, aside of how many packages are on the queue, is looking how much activity there is:

```{r cran-pressure}
subm3 <- cran_times %>%
  arrange(snapshot_time) %>% 
  group_by(package) %>% 
  mutate(autor_change = submission_n != lag(submission_n),
         cran_change = folder != lag(folder)) %>% 
  mutate(autor_change = ifelse(is.na(autor_change), TRUE, autor_change),
         cran_change = ifelse(is.na(cran_change), FALSE, cran_change)) %>% 
  mutate(cran_change = case_when(subfolder != lag(subfolder) ~ TRUE,
                                 TRUE ~ cran_change)) %>% 
  ungroup()
subm3 %>% 
  group_by(snapshot_time) %>% 
  summarize(autor_change = sum(autor_change), cran_change = sum(cran_change)) %>% 
  filter(row_number() != 1) %>% 
  filter(autor_change != 0 | cran_change != 0) %>% 
  ggplot() +
  geom_rect(data = holidays, aes(xmin = start, xmax = end), 
            ymin = -26, ymax = 26, alpha = 0.5, fill = "grey") +
  geom_point(aes(snapshot_time, autor_change), fill = "blue", size = 0) +
  geom_area(aes(snapshot_time, autor_change), fill = "blue") +
  geom_point(aes(snapshot_time, -cran_change), fill = "red", size = 0) +
  geom_area(aes(snapshot_time, -cran_change), fill = "red") +
  scale_x_datetime(date_labels = "%Y/%m/%d", date_breaks = "2 weeks", 
                   expand = expansion(add = 2)) +
  scale_y_continuous(expand = expansion(add = c(0, 0))) + 
  coord_cartesian(ylim = c(-26, 26)) +
  annotate("text", label = "CRAN's", y = 20, x = as_datetime("2020/11/02")) +
  annotate("text", label = "Author's", y = -20, x = as_datetime("2020/11/02")) +
  labs(y = "Changes", x = element_blank(), title = "Activity on CRAN:")
```

On this plot we can see that changes on folders or submissions are not simultaneous.

### Review process {#review-process}

There is a [scheme](https://lockedata.github.io/cransays/articles/dashboard.html#cran-review-workflow) about how does the review process work.
However, it has been pointed out that it needs an update.

We've seen which folders go before which ones, but we haven't looked up what is the last folder in which package appear:

```{r review-process}
last_seen <- cran_times %>% 
  ungroup() %>% 
  group_by(package, submission_n) %>% 
  arrange(snapshot_time) %>% 
  filter(1:n() == last(n())) %>% 
  ungroup()
count(last_seen, folder, sort = TRUE) %>% 
  knitr::kable(col.names = c("Last folder", "Submissions"))
```

We can see that many submissions were left at the pretest, and just a minority on the human or publish folders.

### Time it takes to disappear from the system

One of the motivations to do this post was a [question on R-pkg-devel](https://stat.ethz.ch/pipermail/r-package-devel/2020q4/006174.html), about how long does it usually take for a package to be accepted on CRAN. 
So far we haven't looked at it this way:

```{r cran-public}
package_submissions <- cran_times %>% 
  group_by(package, submission_n) %>% 
  summarise(submission_period = difftime(max(snapshot_time), 
                                         min(snapshot_time), 
                                         units = "hour"),
            submission_time = min(snapshot_time)) %>% 
  ungroup()
```


```{r time-time}
package_submissions %>% 
  # filter(submission_time < holidays$start) %>% 
  ggplot() +
  geom_point(aes(submission_time, submission_period, col = submission_n)) +
  geom_rect(data = holidays, aes(xmin = start, xmax = end),
            ymin = -1, ymax = 3500, alpha = 0.5, fill = "red") + 
  labs(title = "Time on the queue according to the submission",
       x = "Submission", y = "Time (hours)", col = "Submission") +
  theme(legend.position = c(0.5, 0.8))
```

These diagonals suggest that the work is done in batches in a day or an afternoon.
And the prominent diagonal after holidays are packages still on the queue. 

```{r}
package_submissions %>% 
  ungroup() %>%
  mutate(d = as.Date(submission_time)) %>%
  group_by(d) %>% 
  summarize(m = median(submission_period)) %>% 
  ggplot() +
  geom_rect(data = holidays, aes(xmin = as.Date(start), xmax = as.Date(end)),
            ymin = 0, ymax = 50, alpha = 0.5, fill = "red") + 
  geom_smooth(aes(d, m)) +
  coord_cartesian(ylim = c(0, NA)) +
  labs(x = "Submission", y = "Daily median time in queue (hours)", 
       title = "Submission time")
```


```{r}
package_submissions %>% 
  group_by(submission_n) %>% 
  mutate(submission_n = as.character(submission_n)) %>% 
  ggplot() +
  geom_jitter(aes(submission_n, submission_period), height = 0) +
  labs(title = "Submission time in queue", y = "Hours", x = element_blank()) +
  scale_y_continuous(limits = c(1, NA), expand = expansion(add = c(0, 2)))
```

Surprisingly sometimes a submissions goes missing from the folders for some days (I checked with one package I submitted and it doesn't appear for 7 days, although it was on the queue). 
This might affect this analysis as it will count them as new submissions but some of them won't be.


## Github action reliability  {#GHAR}

The data of this post was collected using Github actions by cransays.
At the same time I set up a [bot/package](https://llrs.github.io/cranis/) to scan all the available packages on Bioconductor and CRAN, to track those packages that are accepted and when.
I left it running for 3 months without checking whether I could recover the data.
When I started writing this post I found the data is not usable on the current format.

Instead I'll use cransays data to test how reliable are GitHub actions are.

```{r gha, eval=FALSE, show=FALSE}
download.file("https://github.com/llrs/cranis/archive/history.zip",
              destfile = "static/cranis-history.zip")
path_zip <- here::here("static", "cranis-history.zip") # From downloading the cransays repository branch history
dat <- unzip(path_zip, list = TRUE)
csv <- dat$Name[grepl("*.csv$", x = dat$Name)]
dates <- gsub("available-packages-(.+)\\.csv", "\\1", basename(csv))
date <- as.POSIXct(dates, format = "%Y%m%dT%H%M", "UTC")
gha <- data.frame(
  month = month(date), 
  mday = mday(date), 
  wday = wday(date), 
  week = week(date),
  minute = minute(date), 
  hour = hour(date), 
  type = "cranis")
```


```{r gha2}
gha <- cbind(cran_times[, c("month", "mday", "wday", "week")], 
      minute = minute(cran_submissions$snapshot_time), 
      hour = hour(cran_times$snapshot_time),
      type = "cransays") %>% 
  distinct()
gha %>% 
  ggplot() +
  geom_violin(aes(as.factor(hour), minute)) +
  scale_y_continuous(expand = expansion(add = 0.5), 
                     breaks = c(0, 15, 30, 45, 60), limits = c(0, 60)) +
  scale_x_discrete(expand = expansion())  +
  labs(x = "Hour", y = "Minute", title = "Daily variation")
```

There seems to be a lower limit around 10 minutes except for some builds that I think were manually triggered.
Aside from this, there is usually low variation and the process end  around \~ `r median(gha$minute)` minutes but it can end much later. 
This is just for one simple script scrapping a site.
Compared to thousands of packages builds and checks it is much simpler.

And last how reliable it is?

We can compare how many hours are between the first and the last report and how many do we have recorded.
If we have less this indicate errors on GHA.

```{r reliability, include=FALSE}
diff <- difftime(max(cran_submissions$snapshot_time),
         min(cran_submissions$snapshot_time), units = "hours")
```

So the script and github actions worked on \~`r n_distinct(cran_submissions$snapshot_time)/as.numeric(diff)*100`% of the times. 

These numbers are great, but on CRAN and Bioconductor all packages are checked daily on several OS consistently. 


## Conclusions {#conclusions}

One big difference between CRAN and Bioconductor or rOpenSci is that even if your package is already included each time you want to fix something it gets reviewed by someone.
This ensures a high quality of the packages, as well as increases the work for the reviewers.

The next big difference is the lack of transparency on the process of the review itself.
Perhaps because CRAN started earlier (1997) while Bioconductor in 2002 and rOpenSci much later. 
A public review could help reduce the burden to CRAN reviewers as outsiders could help solving errors (although this is somewhat already fulfilled by the [mailing list](https://www.r-project.org/mail.html) R-package-devel), and would help notice and find a compromise on inconsistencies between reviews. 
As anecdotal evidence I submitted two packages one shortly after the first, on the second package they asked me to change some URLS that on the first I wasn't required to change.

Another difference are the manual page.
It seems that CRAN repository is equated to be R, so the [manual for writing R extensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html) is under `cran.r-project.org` as well as the [CRAN policies](https://cran.r-project.org/web/packages/policies.html). 

The policies change without notice to the existing developers.
Sending an email to the maintainers or R-announce mailing list would help developers to notice policy changes. 
Developers had to create a [policy watch](http://dirk.eddelbuettel.com/blog/2013/10/23/) and other resources to [keep up with CRAN](https://blog.r-hub.io/2019/05/29/keep-up-with-cran/) changes. 

The CRAN reviewers are involved on multiple demanding tasks: their own regular jobs and outside work (familiar, friend, other interests) commitments, and then, R development and maintenance, CRAN reviews and maintenance, R-journal[^4]. 
One possible solution to reduce the burden for them is increase the number of reviewers.
Perhaps a mentorship program to review packages or a guideline to what to check training and would help reduce the pressure on the current volunteers. 

[^4]: I'm not aware of anyone whose full job is just R reviewing. 

Many thanks to all the volunteers that maintain it, those who donate to the R Foundation and the employers of those volunteers that make possible the CRAN and R work.

### Reproducibility

<details>

```{r reproducibility, echo = FALSE}
## Reproducibility info
knitr::clean_cache(TRUE)
options(width = 120)
sessioninfo::session_info()
```

</details>
